\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{subcaption}

\geometry{margin=1in}

% Title
\title{\textbf{Memory-Augmented Transformers via Learnable Cross-Attention Memory Banks}}

\author{
  Tasmay Pankaj Tibrewal\thanks{Equal contribution.} \\
  \texttt{tasmay.tibrewal@fractal.ai} \\
  Fractal AI Research
  \and
  Pritish Saha\footnotemark[1] \\
  \texttt{pritish.saha@kgpian.iitkgp.ac.in} \\
  IIT Kharagpur
  \and
  Ankit Meda\footnotemark[1] \\
  \texttt{ankitm18@kgpian.iitkgp.ac.in} \\
  IIT Kharagpur
}

\date{January 2026}

\begin{document}

\maketitle

\begin{abstract}
Transformers, despite their remarkable success across natural language processing and beyond, lack an explicit architectural mechanism for persistent memory storage and retrieval. Current approaches to memory augmentation predominantly rely on engineering solutions such as KV-caching or retrieval-augmented generation, which operate at inference time rather than at the architectural level. We propose a novel architectural modification: the integration of learnable memory banks accessed via cross-attention mechanisms within transformer blocks. This memory bank consists of latent tokens, randomly initialized and learned during training, which the model can reference as contextual information. We present two complementary research directions: (1) training memory-augmented transformers from scratch, and (2) utilizing memory layers as parameter-efficient adapters for existing pretrained models. We further introduce chapter-based routing inspired by Mixture-of-Experts (MoE) architectures to enable efficient scaling of memory bank size. Additionally, we outline a future direction for dynamic memory updates during inference to enable persistent context retention across conversations. This proposal targets submission to the ICLR 2026 Workshop on New Frontiers in Associative Memories.
\end{abstract}

\section{Introduction}

Transformer architectures \citep{vaswani2017attention} have become the dominant paradigm for sequence modeling, achieving state-of-the-art results across natural language processing, computer vision, and multimodal applications. However, a fundamental limitation persists: transformers do not possess an explicit, architecturally integrated memory layer. Some internal components can be interpreted as storing and retrieving patterns implicitly, for example the feed-forward sublayers behave like key-value memories over training data \citep{geva2021ffnkv}. Nonetheless, an explicit, addressable memory bank may provide a clearer mechanism for controlled capacity scaling and retrieval beyond what is captured implicitly in parameters. While the self-attention mechanism allows tokens to attend to all other tokens within the context window, there is no dedicated component for storing and retrieving knowledge that persists beyond the immediate input sequence. Notably, recent theoretical work has established connections between attention mechanisms and associative memory systems, showing that the softmax attention operation corresponds to the update rule of Modern Hopfield Networks \citep{ramsauer2020hopfield}. Related work on dense associative memories shows that energy-based retrieval can support very large pattern capacity, which motivates adding explicit memory components that scale beyond the immediate context window \citep{krotov2016dense, demircigil2017huge}. This connection suggests that explicit memory mechanisms may be a natural extension of the transformer architecture.

Current solutions to this limitation fall into two broad categories. The first involves engineering approaches such as KV-caching, where key-value pairs from previous tokens are stored and reused during inference to avoid redundant computation. The second encompasses retrieval-augmented generation (RAG) systems \citep{lewis2020rag}, where external documents are retrieved and concatenated to the input context. While these methods have proven effective in practice, they represent applied solutions rather than fundamental architectural innovations. The memory in these systems exists outside the model's learned representations.
 KV cache retention, eviction, and compression schemes provide strong inference-time baselines for long-context use, but do not add a persistent learned memory store \citep{zhang2023h2o, xiao2023streamingllm, karami2025trellis}. For inference-time efficiency, general KV-cache eviction frameworks such as NACL provide important baselines for long-context inference \citep{chen2024nacl}. We view these methods as complementary: cache policies manage attention history, while memory banks provide a learned persistent substrate. Separately, differentiable external-memory architectures such as Neural Turing Machines and Differentiable Neural Computers demonstrate end-to-end learned read/write memory, but were not developed around modern transformer blocks \citep{graves2014ntm, graves2016dnc}.

Several prior works have explored memory augmentation at the architectural level. Memformer \citep{wu2020memformer} introduced cross-attention between layer activations and external memory banks. The Recurrent Memory Transformer (RMT) \citep{bulatov2022rmt} augments transformers with global memory tokens passed between segments to enable recurrence. The Memorizing Transformer \citep{wu2022memorizing} uses approximate kNN lookup into a non-differentiable memory of past key-value pairs. Sandler et al. \citep{sandler2022learnable} demonstrated learnable memory tokens for fine-tuning image transformers. Shah et al. \citep{shah2025metatokens} propose learned meta-tokens as trainable landmarks that improve recall and length generalization. Persistent learned memory vectors integrated directly into attention have also been explored \citep{sukhbaatar2019persistent}, which is closely related in spirit, although our design uses cross-attention to an explicit bank and focuses on scalable routing. Product Key Memory layers \citep{lample2019pkm} showed how to efficiently access large memory banks using product key addressing. More recently, comprehensive surveys \citep{memaugsurvey2025} have systematically categorized these approaches according to functional objectives, memory representations, and integration mechanisms.

In this proposal, we present an architectural modification that introduces a learnable memory bank integrated via cross-attention. Unlike inference-time memory solutions, our memory bank is learned during training, allowing the model to develop compressed, efficient representations of knowledge. We propose two research tracks: (1) training memory-augmented models from scratch, and (2) using memory layers as parameter-efficient adapters. We further introduce MoE-inspired chapter-based routing to scale memory efficiently, and outline future work on dynamic memory updates during inference.

\section{Idea Overview}

\noindent \textbf{Base proposal:} Introduce an explicit, architectural memory component for transformers, rather than relying only on inference-time mechanisms such as extending the context window, KV-cache policies, or external retrieval pipelines \citep{lewis2020rag}.

\smallskip
\noindent \textbf{Objective:} Standard transformers do not expose an explicit, addressable memory layer. Our goal is to increase effective knowledge retention and controllable capacity by adding a persistent memory substrate within the architecture.

\smallskip
\noindent \textbf{Core change (learnable memory bank):} Maintain a memory bank of latent tokens, randomly initialized and optimized end-to-end during training. For each forward pass, the token stream produces queries ($\mathbf{Q}$), while the memory bank provides keys and values ($\mathbf{K}, \mathbf{V}$). Cross-attention then retrieves relevant information from this persistent latent store, enabling memory access without requiring the information to be present in the input sequence.

\smallskip
\noindent \textbf{Integration into a transformer block (variants):}
\begin{itemize}[noitemsep]
    \item \textbf{Variant A:} Self-attention $\rightarrow$ memory cross-attention $\rightarrow$ MLP.
    \item \textbf{Variant B:} Self-attention $\rightarrow$ MLP $\rightarrow$ memory cross-attention $\rightarrow$ MLP.
    \item Memory layers need not appear in every block; we can place them in the first $k$ layers, last $k$ layers, every $n$-th layer, or any other sparse schedule.
\end{itemize}

\smallskip
\noindent \textbf{Shared vs. per-layer memory:}
\begin{itemize}[noitemsep]
    \item \textbf{Per-layer banks:} smaller bank per memory-enabled layer (supports layer-specific information specialization).
    \item \textbf{Shared bank:} one larger bank used by many layers (increases accessible information per access, with potential interference from attending to less relevant tokens).
    \item Hypothesis: even if layers operate in slightly different representational spaces, layer-specific memory projections can learn the necessary alignment jointly with the key/value mapping.
\end{itemize}

\smallskip
\noindent \textbf{Scaling memory access (chapters + routing):} Since cross-attention cost grows linearly with the number of memory tokens, we partition the bank into ``chapters'' and train a lightweight router (MoE-style \citep{shazeer2017moe, fedus2022switch}) to select top-$k$ chapters per input. Each memory-enabled layer then attends only to routed chapters (e.g., 100k tokens split into 100 chapters; attending to top 20 chapters results in 20k tokens per access).

\smallskip
\noindent \textbf{Token-wise routing (challenge and opportunity):} In MoE, routing is typically token-wise. Here, token-wise chapter routing during prefill/training is memory-prohibitive because it effectively requires per-token routed $\mathbf{K},\mathbf{V}$ materialization, scaling with sequence length. A promising systems direction is a custom matmul/CUDA kernel that stores only \emph{unique} routed chapters and uses an index map so many query tokens can reuse the same chapter rows without redundant copies. Token-wise routing becomes substantially more feasible during autoregressive generation (sequence length $=1$), but may increase VRAM usage because routed memory must coexist with the KV cache.

\smallskip
\noindent \textbf{Increasing bank size under VRAM constraints:}
\begin{itemize}[noitemsep]
    \item \textbf{Quantized storage:} store memory tokens in low precision.
    \item \textbf{Low-rank structure} (LoRA-inspired \citep{hu2022lora}): factorize the bank, or store memory directly in a reduced dimension and perform memory attention in the reduced space before projecting back.
    \item The same techniques apply both to the training-time memory bank and to the inference-time context bank described below.
\end{itemize}

\smallskip
\noindent \textbf{Mitigating stage-wise forgetting:} When moving from pretraining $\rightarrow$ instruction fine-tuning, protect stored knowledge by freezing the memory bank (and optionally memory projections), or by applying lower learning rates to memory parameters than to other trainable components.

\smallskip
\noindent \textbf{Experimental plan (workshop-feasible):}
\begin{itemize}[noitemsep]
    \item \textbf{From-scratch track:} Train a smaller model with memory from scratch and compare against a same-size baseline transformer, testing whether explicit memory improves retention and capacity.
    \item \textbf{Adapter track (primary):} Use memory layers as PEFT modules on a pretrained model; compare full fine-tuning vs LoRA vs Memory-Adapters vs Memory-Adapters+LoRA. Evaluate on reasoning traces and/or domain datasets (medical/legal), where retention and controlled updates are central.
\end{itemize}

\smallskip
\noindent \textbf{Future extension (dynamic inference-time context bank):} In addition to the learned (static) memory bank, maintain a separate \emph{context bank} that stores information encountered during inference, enabling long-horizon agents and cross-conversation memory. Outline:
\begin{itemize}[noitemsep]
    \item \textbf{Compression:} compress recent prompts into a small number of latent vectors (e.g., VAE/autoencoder over early+late layer embeddings).
    \item \textbf{Retrieval without a trained router:} cluster context vectors (k-means/hierarchical/online; IVF-PQ at scale) and attend only to top-$k$ clusters selected by similarity to the current query.
    \item \textbf{Capacity management:} when the bank reaches capacity, merge nearby vectors to free space, optionally guided by usage-derived importance (how often/strongly attended) and recency/age, yielding a controlled forgetting rule.
    \item \textbf{Separation of concerns:} keep the learned memory bank and inference-time context bank separate so post-deployment updates do not interfere with training-time knowledge.
\end{itemize}

\section{Proposed Architecture}

\subsection{Core Memory Bank Design}

We propose augmenting the standard transformer block with a cross-attention layer that enables prompt tokens to attend to a set of learnable memory tokens. Let $\mathbf{M} \in \mathbb{R}^{N_m \times d}$ denote the memory bank, where $N_m$ is the number of memory tokens and $d$ is the embedding dimension. This memory bank is randomly initialized and updated through gradient descent during training.

Architecturally, this follows the general pattern of cross-attention from the token stream to a fixed latent array, as used in Perceiver-style models, although here the latent array is trained to act as a persistent knowledge store \citep{jaegle2021perceiver, jaegle2021perceiverio}. Persistent learned memory vectors inserted into self-attention are related in spirit \citep{sukhbaatar2019persistent}, but our approach uses a separate bank accessed via cross-attention rather than appending persistent tokens to the self-attention context. This is also closely related in form to external-memory attention mechanisms used for long-range sequence modeling \citep{wu2020memformer}.

Given a sequence of prompt tokens with hidden representations $\mathbf{H} \in \mathbb{R}^{L \times d}$ (where $L$ is the sequence length), the cross-attention operation proceeds as follows:

\begin{align}
\mathbf{Q} &= \mathbf{H} \mathbf{W}_Q \\
\mathbf{K} &= \mathbf{M} \mathbf{W}_K \\
\mathbf{V} &= \mathbf{M} \mathbf{W}_V \\
\text{MemAttn}(\mathbf{H}, \mathbf{M}) &= \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^\top}{\sqrt{d_k}}\right) \mathbf{V}
\end{align}

where $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V \in \mathbb{R}^{d \times d_k}$ are learnable projection matrices and $d_k$ is the head dimension.

\subsection{Transformer Block Variants}

We consider two variants for integrating the memory cross-attention layer within a transformer block:

\textbf{Variant A (Post-Self-Attention):}
\begin{enumerate}[noitemsep]
    \item Self-attention layer
    \item Memory cross-attention layer
    \item MLP layer
\end{enumerate}

\textbf{Variant B (Post-MLP):}
\begin{enumerate}[noitemsep]
    \item Self-attention layer
    \item MLP layer
    \item Memory cross-attention layer
    \item Additional MLP layer
\end{enumerate}

Variant B provides an additional non-linear transformation after memory retrieval, potentially enabling more complex integration of retrieved information.

\subsection{Memory Layer Placement Strategies}

Rather than placing memory cross-attention in every transformer block, selective placement may prove more efficient:

\begin{itemize}[noitemsep]
    \item \textbf{First-$k$ layers:} Memory reference in the first $k$ layers, where syntactic and lower-level information is typically processed.
    \item \textbf{Last-$k$ layers:} Memory reference in the final $k$ layers, where higher-level semantic information is integrated.
    \item \textbf{Every-$n$th layer:} Periodic memory reference (e.g., every 3rd layer).
    \item \textbf{All layers:} Full memory integration throughout the network.
\end{itemize}

The optimal placement strategy likely depends on the type of information stored in the memory bank and the downstream task requirements.

\subsection{Shared vs. Per-Layer Memory Banks}

Two memory bank configurations are possible:

\textbf{Per-Layer Memory Banks:} Each memory-enabled layer maintains its own memory bank $\mathbf{M}^{(l)} \in \mathbb{R}^{N_m^{(l)} \times d}$. This allows different layers to store information at different levels of abstraction, analogous to how different transformer layers capture different linguistic phenomena.

\textbf{Shared Memory Bank:} A single memory bank $\mathbf{M} \in \mathbb{R}^{N_m \times d}$ is shared across all memory-enabled layers. This configuration provides each layer access to a larger pool of information but raises a concern: different layers operate in slightly different representational vector-spaces (linear subspaces).

We address this vector-space mismatch concern as follows. The projection matrices $\mathbf{W}_K^{(l)}$ and $\mathbf{W}_V^{(l)}$ at each layer $l$ can learn not only the key/value transformations but also the vector-space transformation between the shared memory space and the layer-specific representation space. Since these projection matrices have dimension $d \times d_k$, and a linear transformation of this size is sufficient to map between any two $d$-dimensional vector-spaces, the layer-specific projections can absorb the vector-space alignment task without requiring additional parameters.

Formally, we can conceptualize this as:
\begin{align}
\mathbf{W}_K^{(l)} = \mathbf{W}_K^{\text{core}} \cdot \mathbf{T}_K^{(l)}\\
\mathbf{W}_V^{(l)} = \mathbf{W}_V^{\text{core}} \cdot \mathbf{T}_V^{(l)}
\end{align}
where $\mathbf{W}_K^{\text{core}}$ performs the standard key projection and $\mathbf{T}_K^{(l)}$ handles vector-space transformation. However, since both are learnable and their product is also a $d \times d_k$ matrix, a single learned $\mathbf{W}_K^{(l)}$ can capture both functionalities.

The shared memory configuration has the advantage of providing each layer access to a larger information pool. A potential downside is that different layers may require different types of information (e.g., syntactic information in early layers, factual knowledge in middle layers), and shared memory may force layers to attend to irrelevant tokens. We propose to investigate this trade-off empirically.

\section{Scaling Memory via Chapter-Based Routing}

\subsection{Motivation}

A key challenge in memory-augmented transformers is scaling the memory bank size. With $N_m$ memory tokens and sequence length $L$, the cross-attention computation scales as $O(L \cdot N_m)$. For large memory banks (e.g., $N_m = 100,000$ tokens), this becomes computationally prohibitive.

Related approaches scale memory by using sparse top-$k$ retrieval rather than dense attention, for example product-key addressing for large memory layers and MIPS-style memory querying \citep{lample2019pkm, wu2022emat}, and recent work shows that trainable memory layers can be scaled effectively with strong factual gains \citep{berges2025memorylayers}.

We address this challenge by introducing chapter-based routing, inspired by Mixture-of-Experts (MoE) architectures \citep{shazeer2017moe, fedus2022switch}. Rather than attending to all memory tokens, we partition the memory bank into ``chapters'' and use a router to select the most relevant chapters for each input.

This is aligned with content-based sparse attention methods that route queries to a small subset of candidates, for example by clustering tokens and restricting attention to routed groups \citep{roy2021routing}.

This goal also aligns with scalable long-term memory extensions that select a small subset of a large internal memory at each step \citep{wang2025mplus}.

Relatedly, MoM formulates sequence modeling with a mixture of memory components combined by learned routing or gating, offering a useful precedent for selectively accessing multiple memory slots rather than treating memory as monolithic \citep{du2025mom}. Neural Routing by Memory likewise studies routing decisions driven by a memory state, supporting the general design pattern of memory-conditioned module selection \citep{zhang2021neuralrouting}. In contrast, our emphasis is on augmenting Transformer blocks with learnable cross-attention memory banks and optional chapter-level routing, keeping the base self-attention backbone intact.

Notably, memory chapter routing can achieve much higher sparsity than typical MoE expert routing. While MoE systems distribute computational operations (MLP transformations) across experts and must maintain reasonable coverage for diverse token types, memory queries are inherently more specific: a given query typically requires access to a small, targeted subset of knowledge rather than diverse computational pathways. This allows for sparser routing (e.g., selecting 10-20 out of 1000 chapters) without degrading performance, as queries naturally align with specific memory regions rather than requiring broad computational diversity.

\subsection{Chapter Organization}

Let the memory bank be partitioned into $C$ chapters, each containing $N_c = N_m / C$ tokens:
\begin{align}
\mathbf{M} = [\mathbf{M}_1; \mathbf{M}_2; \ldots; \mathbf{M}_C]
\end{align}
where $\mathbf{M}_c \in \mathbb{R}^{N_c \times d}$ denotes the $c$-th chapter.

\subsection{Router Architecture}

The router computes chapter importance scores based on the input sequence. Let $\bar{\mathbf{h}} = \frac{1}{L} \sum_{i=1}^{L} \mathbf{h}_i$ be the mean-pooled representation of the input sequence. The router computes:
\begin{align}
\mathbf{s} = \text{softmax}(\mathbf{W}_r \bar{\mathbf{h}} + \mathbf{b}_r)
\end{align}
where $\mathbf{W}_r \in \mathbb{R}^{C \times d}$ and $\mathbf{b}_r \in \mathbb{R}^C$ are learnable parameters, and $\mathbf{s} \in \mathbb{R}^C$ contains the importance scores for each chapter.

We select the top-$k$ chapters with highest scores and perform cross-attention only over the tokens in these selected chapters. If $k = 20$ chapters are selected from $C = 100$ chapters (each containing $N_c = 1,000$ tokens), we attend to only $20,000$ tokens instead of $100,000$, achieving a $5\times$ reduction in attention computation.

\subsection{Token-Level Routing Challenges}

Ideally, routing would be performed at the token level, allowing different query tokens to attend to different memory chapters. This mirrors the token-level expert assignment in standard MoE \citep{zhou2022moe}. However, token-level routing presents significant memory challenges.

Consider the tensor dimensions for standard cross-attention with sequence-level routing:
\begin{align}
\mathbf{Q} &\in \mathbb{R}^{B \times H \times L \times D} \\
\mathbf{K} &\in \mathbb{R}^{B \times H \times N_{\text{routed}} \times D}
\end{align}
where $B$ is batch size, $H$ is number of heads, $L$ is sequence length, $D$ is head dimension ($D=d_k$), and $N_{\text{routed}}$ is the number of routed memory tokens.

For token-level routing, where each token can route to different chapters, the key tensor must expand to accommodate per-token routing:
\begin{align}
\mathbf{K}_{\text{token-level}} \in \mathbb{R}^{(B \times L) \times H \times N_{\text{routed}} \times D}
\end{align}

To enable this with standard matrix multiplication, we would need to reshape the query tensor to match. The reshaping sequence would be:
\begin{align}
\mathbf{Q} &\in \mathbb{R}^{B \times H \times L \times D} \nonumber \\
&\xrightarrow{\text{transpose}(1,2)} \mathbb{R}^{B \times L \times H \times D} \nonumber \\
&\xrightarrow{\text{view}(0,1)} \mathbb{R}^{(B \times L) \times H \times D} \nonumber \\
&\xrightarrow{\text{unsqueeze}(3)} \mathbb{R}^{(B \times L) \times H \times D \times 1} \nonumber \\
&\xrightarrow{\text{transpose}(2,3)} \mathbb{R}^{(B \times L) \times H \times 1 \times D}
\end{align}

Now $\mathbf{Q}$ and $\mathbf{K}^\top$ would be compatible for matrix multiplication, producing attention scores of shape $(B \times L) \times H \times 1 \times N_{\text{routed}}$, which can be reshaped back to $B \times L \times H \times N_{\text{routed}}$ for the attention computation.

However, this approach is infeasible due to memory requirements. For practical values ($B = 250$, $L = 10,000$, $H = 32$, $D = 128$, $N_{\text{routed}} = 16,000$), the key tensor would require approximately 300 TB of memory, which is clearly prohibitive.

We identify a potential solution through custom CUDA kernels. Rather than materializing the full key tensor, a custom matrix multiplication kernel could:
\begin{enumerate}[noitemsep]
    \item Store only the unique chapters in memory: $\mathbf{K}_{\text{unique}} \in \mathbb{R}^{C_{\text{unique}} \times H \times N_c \times D}$ where $C_{\text{unique}}$ is the number of unique chapters and $N_c$ is tokens per chapter
    \item Maintain a routing index mapping each query token to its assigned chapters (approximately 160 MB for 40 million references, approximately negligible)
    \item Perform attention by referencing the same chapter data for multiple queries, avoiding redundant storage
\end{enumerate}

This would reduce memory from $O(B \cdot L \cdot N_{\text{routed}})$ to $O(C_{\text{unique}} \cdot N_c)$ plus routing indices. For a memory bank of 1 million tokens divided into 1000 chapters of 1000 tokens each, this yields approximately 8 GB for the chapters, a reduction of approximately 40,000$\times$ compared to the na\"ive approach. However, implementing this custom kernel requires significant CUDA expertise and careful optimization to maintain parallelization efficiency.

\textbf{Token-Level Routing During Generation:} While token-level routing is infeasible during prefill (processing the entire input sequence) and training due to memory constraints, it becomes viable during autoregressive generation. During generation, we process one token at a time (sequence length $L = 1$), which naturally eliminates the multiplicative $L$ factor in memory requirements. In this regime, token-level routing can be performed efficiently, though it requires storing the routed memory tokens in VRAM alongside the KV cache. For example, routing to 50,000 memory tokens per generation step would add approximately 200 MB per sample to VRAM (at full precision for a 4096-dimensional model). This overhead could be further reduced by the custom CUDA kernel approach described above, limiting total memory to the size of the memory bank itself rather than per-token copies.

Given these challenges, we leave token-level routing during prefill as future work and proceed with sequence-level routing for our workshop submission, while noting that token-level routing during generation remains a practical option.

\section{Memory Adapters for Efficient Fine-Tuning}

\subsection{Motivation}

Given the timeline constraints for the ICLR workshop (deadline: February 14, 2026), pretraining a memory-augmented transformer from scratch and conducting comprehensive experiments may not be feasible within approximately three weeks. We therefore propose an alternative approach: using memory layers as parameter-efficient adapters for existing pretrained models.

This approach offers several advantages:
\begin{itemize}[noitemsep]
    \item Leverages the knowledge already encoded in pretrained weights
    \item Requires training only the memory bank and associated projection matrices
    \item Enables direct comparison with established PEFT methods (e.g., adapters \citep{houlsby2019adapters} and prefix-tuning \citep{li2021prefix})
    \item Includes LoRA as a standard low-rank adaptation baseline \citep{hu2022lora}
    \item Provides comparison against strong prompt-based tuning baselines such as P-Tuning v2 \citep{liu2021ptuningv2}
    \item Reduces computational requirements for experiments
\end{itemize}

Recent work also treats memory systems through the lens of parameter-efficient adapters. MemLoRA distills expert adapters for on-device memory systems, providing a concrete reference point for memory as a PEFT module design choices \citep{bini2025memlora}. Our approach is orthogonal: we introduce learnable cross-attention memory banks (and optional routing), which can coexist with adapter-based fine-tuning.

\subsection{Memory Adapter Architecture}

For a pretrained transformer, we insert memory cross-attention layers at selected positions while keeping the original weights frozen. The trainable parameters consist of:
\begin{itemize}[noitemsep]
    \item Memory bank $\mathbf{M} \in \mathbb{R}^{N_m \times d}$
    \item Projection matrices $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V \in \mathbb{R}^{d \times d_k}$ per memory layer
    \item (Optional) Router parameters for chapter-based routing
\end{itemize}

\subsection{Memory Compression Techniques}

To reduce the parameter count and memory footprint of memory adapters, we propose several compression strategies:

\textbf{Quantized Memory Banks:} Store memory tokens in lower precision (e.g., 4-bit or 8-bit) while maintaining full precision for computations.

\textbf{Low-Rank Memory Factorization:} We can decompose the memory bank into lower-rank factors:
\begin{align}
\mathbf{M} = \mathbf{A} \mathbf{B}^\top
\end{align}
where $\mathbf{A} \in \mathbb{R}^{N_m \times r}$ and $\mathbf{B} \in \mathbb{R}^{d \times r}$ with $r \ll \min(N_m, d)$.

This reduces the parameter count from $N_m \cdot d$ to $(N_m + d) \cdot r$. However, this comes at the cost of reduced expressiveness, as each memory token can now store less information. This technique is particularly valuable for the memory adapter approach, as it dramatically reduces the number of trainable parameters that need to be added to a frozen pretrained model, making memory adapters more parameter-efficient and competitive with other PEFT approaches.

\textbf{Low-Rank Projections:} The projection matrices $\mathbf{W}_Q, \mathbf{W}_K, \mathbf{W}_V$ can also be factorized:
\begin{align}
\mathbf{W}_Q = \mathbf{W}_Q^{\text{down}} \mathbf{W}_Q^{\text{up}}
\end{align}
where $\mathbf{W}_Q^{\text{down}} \in \mathbb{R}^{d \times r}$ and $\mathbf{W}_Q^{\text{up}} \in \mathbb{R}^{r \times d_k}$. This projection factorization further reduces the adapter parameter count and can be combined with memory bank factorization for maximum parameter efficiency.

\textbf{Low-Rank Dimension Reduction:} An alternative approach is to store the memory bank directly in reduced dimensions $\mathbf{M} \in \mathbb{R}^{N_m \times r}$ where $r \ll d$ (e.g., $r = 512$ instead of $d = 4096$). The projection matrices then map between the full model dimension and the reduced memory dimension:
\begin{align}
\mathbf{W}_Q &\in \mathbb{R}^{d \times r} \quad \text{(maps queries from } d \text{ to } r \text{)} \\
\mathbf{W}_K, \mathbf{W}_V &\in \mathbb{R}^{r \times r} \quad \text{(operate in reduced space)} \\
\mathbf{W}_O &\in \mathbb{R}^{r \times d} \quad \text{(maps outputs back to } d \text{)}
\end{align}

The cross-attention computation becomes:
\begin{align}
\mathbf{Q}_r &= \mathbf{H} \mathbf{W}_Q \in \mathbb{R}^{L \times r} \\
\mathbf{K}_r &= \mathbf{M} \mathbf{W}_K \in \mathbb{R}^{N_m \times r} \\
\mathbf{V}_r &= \mathbf{M} \mathbf{W}_V \in \mathbb{R}^{N_m \times r} \\
\text{MemAttn}_r(\mathbf{H}, \mathbf{M}) &= \text{softmax}\left(\frac{\mathbf{Q}_r\mathbf{K}_r^\top}{\sqrt{r}}\right) \mathbf{V}_r \mathbf{W}_O
\end{align}

This approach reduces memory bank storage by a factor of $d/r$ (e.g., 8$\times$ reduction for $r=512$, $d=4096$) while performing all attention computations in the lower-dimensional space. The output projection $\mathbf{W}_O$ maps the result back to the full model dimension for integration with subsequent layers. This trades expressiveness for efficiency: each memory token stores less information, but the dramatic reduction in storage enables much larger memory banks within the same memory budget. For memory adapters specifically, this dimension reduction approach is especially advantageous: it allows deploying memory adapters with substantially larger memory banks (potentially 100k+ tokens) on resource-constrained hardware, making the adapter approach practical for real-world fine-tuning scenarios where VRAM is limited.

\subsection{Proposed Experiments}

We propose the following experimental comparisons using a base model such as Qwen-2.5-1.5B \citep{qwen2} or Qwen-2.5-Math-1.5B:

\begin{enumerate}[noitemsep]
    \item \textbf{Full Fine-Tuning:} Update all model parameters (baseline)
    \item \textbf{Adapters:} Inserted adapter modules \citep{houlsby2019adapters}
    \item \textbf{Prefix-Tuning:} Learned prefix prompts \citep{li2021prefix}
    \item \textbf{P-Tuning v2:} Deep prompt tuning baseline \citep{liu2021ptuningv2}
    \item \textbf{LoRA:} Low-rank adaptation baseline \citep{hu2022lora}
    \item \textbf{Sparse Memory PEFT:} Hierarchical sparse memory baselines such as SPARTAN \citep{deshpande2022spartan}
    \item \textbf{Memory Adapters (Ours):} Memory bank with cross-attention
    \item \textbf{Memory Adapters + LoRA:} Memory bank with cross-attention combined with LoRA
\end{enumerate}

\textbf{Datasets:} We consider fine-tuning on:
\begin{itemize}[noitemsep]
    \item DeepSeek-R1 reasoning traces from Open-R1 math dataset (to evaluate whether memory can capture reasoning patterns)
    \item Domain-specific datasets (medical, legal) where factual knowledge retention is critical
\end{itemize}

\textbf{Evaluation:} Performance will be measured on held-out test sets, with particular attention to:
\begin{itemize}[noitemsep]
    \item Task accuracy
    \item Parameter efficiency (trainable parameters vs. performance)
    \item Inference speed overhead from memory attention
\end{itemize}

\subsection{Preventing Knowledge Loss During Fine-Tuning}

A known issue with fine-tuning is catastrophic forgetting, where the model loses information acquired during pretraining. Memory adapters may help mitigate this issue, as the pretrained weights remain frozen. Related discrete adaptor and bottleneck approaches aim to localize updates into a small set of key-value slots, reducing interference during continual edits \citep{hartvigsen2023grace, trauble2023dkvb}. Additionally, we propose:

\textbf{Memory Freezing:} After pretraining (for from-scratch models) or an initial fine-tuning phase, freeze the memory bank and associated $\mathbf{W}_K, \mathbf{W}_V$ matrices while continuing to train other components.

\textbf{Differential Learning Rates:} Apply lower learning rates to memory bank parameters compared to other trainable parameters, reducing the rate at which memorized information is overwritten.

\section{From-Scratch Training}

While memory adapters are our primary focus for the workshop submission, we also outline plans for training memory-augmented transformers from scratch, if possible in time.

\subsection{Proposed Setup}

\begin{itemize}[noitemsep]
    \item \textbf{Model Size:} Approximately 100M parameters
    \item \textbf{Pretraining Data:} 5-10 billion tokens
    \item \textbf{Instruction Fine-Tuning:} 10-100M tokens
    \item \textbf{Memory Configuration:} 10,000-1M memory tokens (shared or per-layer variants)
\end{itemize}

\subsection{Expected Outcomes}

We hypothesize that the learned memory bank will contain more compressed and efficient representations compared to:
\begin{itemize}[noitemsep]
    \item Textual memory (as in retrieval-augmented systems)
    \item Non-differentiable memory stores (as in Memorizing Transformers \citep{wu2022memorizing})
    \item Inference-time memory (as in TransformerFAM \citep{hwang2024transformerfam})
\end{itemize}

The model should be compared against a baseline transformer of equivalent parameter count (excluding memory bank) on standard language modeling benchmarks.

\section{Dynamic Memory Update During Inference (Future Work)}

\subsection{Motivation}

The memory mechanisms described thus far are static: the memory bank is fixed after training and does not update during inference. This limits the model's ability to incorporate new information encountered at inference time.

This goal is also related to online adaptation methods that maintain a memory of amortized contexts during deployment \citep{tack2024mac}.

We propose extending the architecture with a dynamic ``context bank'' that is updated during inference, enabling:
\begin{itemize}[noitemsep]
    \item Long-context retention beyond the attention window (current models effectively handle up to approximately 400,000 tokens before losing coherence; existing techniques like context compression and selective memory documentation mitigate this but still face limitations for autonomous agents running for hours or days)
    \item Cross-conversation memory (remembering information across sessions)
    \item Personal agents that accumulate knowledge about users over time
    \item Autonomous agents operating continuously over extended periods (hours to days) with efficient, precise long-term memory representation
    \item Rapid knowledge updates on recent news or important events without retraining
\end{itemize}

Recent work on long-term and episodic memory for language models studies related goals such as retaining information across extended interactions and controlling what is stored and retrieved \citep{wang2023longtermemory, liu2024larimar}. Test-time memory mechanisms that learn what to store during inference provide an additional point of comparison for dynamic updates \citep{behrouz2025titans}. Compressed context memory for online language model interaction addresses a closely related setting where a model maintains and retrieves a compact interaction memory over time \citep{kim2024ccm}.

Episodic test-time memory is an active direction. EM-LLM introduces an episodic memory mechanism for large language models, storing and retrieving past episodes to improve long-horizon consistency \citep{fountas2025emllm}. This is closely adjacent to our Dynamic Context Bank discussion; however, our main contribution remains architectural memory via learnable cross-attention banks, and our dynamic memory sketches emphasize alternative compression and structuring choices rather than purely episodic retrieval.

This architectural approach offers several advantages over existing memory systems. Unlike current ChatGPT-like memory implementations that rely on tool-based text storage and selective text retrieval, our context bank operates at the architectural level with learned latent representations. This enables the model to remember fine-grained details more clearly and precisely, as the memory is embedded in the model's representational space rather than stored as unstructured text that must be re-encoded at each use.

\subsection{Context Bank Architecture}

We maintain a separate context bank $\mathbf{C} \in \mathbb{R}^{N_{ct} \times d}$ that stores compressed representations of inference-time inputs. The context bank is distinct from the memory bank to prevent training-time knowledge from being overwritten.

\textbf{Compression:} Input sequences are compressed before storage using a VAE-style encoder. This direction is complementary to recent context compression modules that produce compact representations for long-context inference \citep{activationbeacon2024, icformer2024, icae2023}. Learned context auto-compressors for language models are a related direction \citep{chevalier2023autocompressors}. Autoencoding-free context compression via contextual semantic anchors is another related approach that targets efficient long-context inference without a separate reconstruction model \citep{liu2025semanticanchors}. This line of work is relevant for any Dynamic Context Bank variant that must retain salient information under tight context or KV-cache budgets. Given input tokens with embeddings from the first layer $\mathbf{E}^{(1)} \in \mathbb{R}^{L \times d}$ and last layer $\mathbf{E}^{(L_{\text{max}})} \in \mathbb{R}^{L \times d}$, we train an encoder:
\begin{align}
\mathbf{z} = \text{Encoder}([\mathbf{E}^{(1)}; \mathbf{E}^{(L_{\text{max}})}])
\end{align}
where $\mathbf{z} \in \mathbb{R}^{L_c \times d}$ with $L_c \ll L$ (e.g., compressing 10,000 tokens to 100 tokens).

The encoder is trained to reconstruct the concatenated embeddings, ensuring that both raw semantic information (from early layers) and processed information (from late layers) are preserved.

\textbf{Alternative: Time Series VAE:} Instead of treating the token sequence as independent embeddings, we can train a time series VAE that explicitly models temporal dependencies in the sequence. This approach captures the sequential structure and dependencies between tokens, potentially enabling better compression of coherent spans of text (e.g., complete sentences or paragraphs) by leveraging their temporal structure. The time series VAE would use recurrent or convolutional encoders to process the sequence temporally before compressing it to the latent representation.

\textbf{Storage:} Compressed representations $\mathbf{z}$ are appended to the context bank until a maximum capacity $N_{ct}^{\text{max}}$ is reached.

\textbf{Retrieval:} Since attending to a large context bank (e.g., 1M tokens) is computationally prohibitive, we use clustering-based retrieval (clusters are analogous to chapters here, but since we are adding tokens in test-time, we cannot have a trained router for retrieval and thus we cluster similar tokens together for retrieval), similar to approaches in efficient attention literature \citep{retrieval2024}:
\begin{enumerate}[noitemsep]
    \item Cluster context bank tokens using k-means or hierarchical clustering
    \item For a new query, compute dot products with cluster centroids
    \item Select top-$k$ clusters and attend only to tokens within those clusters
\end{enumerate}

This approach requires no training at inference time, as cluster selection is based on embedding similarity rather than learned routing.

\textbf{Scalable Clustering Methods:} For very large context banks, standard k-means may become computationally expensive. We can employ more scalable techniques:
\begin{itemize}[noitemsep]
    \item \textbf{Hierarchical clustering:} Build a tree structure enabling logarithmic-time search for relevant clusters
    \item \textbf{Online k-means:} Update cluster centroids incrementally as new tokens are added, avoiding full re-clustering
    \item \textbf{IVF-PQ (Inverted File with Product Quantization):} Use inverted index structures with quantized representations for memory-efficient storage and fast approximate nearest neighbor search
\end{itemize}

These methods enable efficient addition of new tokens to the context bank and fast retrieval even as the bank scales to millions of tokens.

\subsection{Memory Consolidation}

When the context bank reaches capacity and new information must be stored, we propose a consolidation mechanism inspired by how humans forget information:

\textbf{Vector Merging:} Identify pairs of vectors with highest cosine similarity and merge them via addition or averaging:
\begin{align}
\mathbf{c}_{\text{merged}} = \frac{\mathbf{c}_i + \mathbf{c}_j}{2}
\end{align}

This is inspired by RAG systems where chunk embeddings are averaged without catastrophic information loss for semantic matching.

\textbf{Importance-Weighted Merging:} Incorporate additional factors when selecting which vectors to merge:
\begin{itemize}[noitemsep]
    \item \textbf{Reference frequency:} How often a vector has been attended to
    \item \textbf{Attention importance:} Average softmax weight when attended
    \item \textbf{Recency (Age):} Newer information is retained longer (benefit of doubt for importance). Age is measured as the number of inference runs (context bank updates) since the vector was added, where one run corresponds to processing one prompt through the model. This provides a discrete timestep measure of how long information has been stored.
\end{itemize}

A scoring function could weight these factors:
\begin{align}
\text{importance}(\mathbf{c}) = \alpha \cdot \text{freq}(\mathbf{c}) + \beta \cdot \text{avg\_attn}(\mathbf{c}) + \gamma \cdot \text{recency}(\mathbf{c})
\end{align}

where recency is inversely related to age (recent tokens have low age values). Vectors with low importance scores are prioritized for merging.

\textbf{Note:} All compression techniques (low-rank, quantization) described for the memory bank can also be applied to the context bank.

\section{Relation to Prior Work}

Our proposal builds upon and differs from several lines of prior research. We provide a comprehensive comparison to position our contributions.

\textbf{Associative Memory and Hopfield Networks:} Classical Hopfield networks \citep{hopfield1982} provided the foundational framework for associative memory in neural networks. Subsequent work on Modern Hopfield and dense associative memory models established that these systems can achieve very large (in some settings, exponential) storage capacity \citep{krotov2016dense, demircigil2017huge}. Ramsauer et al. \citep{ramsauer2020hopfield} then showed that the attention mechanism in transformers is mathematically equivalent to the update rule of Modern Hopfield Networks, providing theoretical grounding for viewing attention as associative retrieval. Our work extends this view by adding an explicit, learnable memory bank that expands the set of patterns available for retrieval.

\textbf{Cross-Attention Memory with Dynamic Updates:} Several works have explored cross-attention to memory banks. Memformer \citep{wu2020memformer} introduced cross-attention between layer activations and external memory banks using the same Q/K/V formulation as our approach (input provides Q, memory provides K,V) but with gated updates that modify memory during the forward pass. Most recently, LM2 (Large Memory Models) \citep{kang2025lm2} proposed an auxiliary memory module with cross-attention and LSTM-style gating (input, output, forget gates) for dynamic memory updates. Stateful Memory-Augmented Transformers \citep{wu2022stateful} apply similar ideas to dialogue modeling with recurrent memory states. MEMORYLLM \citep{wang2024memoryllm} is representative of self-updatable language models that maintain an internal latent memory pool during deployment, and M+ extends this direction toward scalable long-term memory \citep{wang2025mplus}. In this workshop submission, we focus primarily on static learned memory banks with chapter routing and a PEFT memory-adapter instantiation, while treating dynamic updates as future work. Our key distinction is that our memory bank is \textbf{static after training}: it stores persistent learned knowledge rather than working memory that evolves during inference. This design choice targets knowledge retention rather than context tracking.

\textbf{Conditional Memory and Hash-Based Lookup:} DeepSeek's Engram \citep{deepseek2026engram} introduces conditional memory as a complementary sparsity axis to MoE, using hash-based O(1) lookup for static token patterns (common phrases, entities). While Engram and our approach share the goal of separating memory from computation, the mechanisms differ fundamentally: Engram uses deterministic hash addressing for specific n-grams, while our attention-based approach enables soft retrieval with learned content-based addressing. Our chapter routing provides semantic organization, whereas Engram's hashing is pattern-based.

\textbf{Product Key Memory and Memory Layers:} Lample et al. \citep{lample2019pkm} introduced Product Key Memory layers that enable efficient access to very large memory banks using product key addressing. More recently, Memory Layers at Scale \citep{berges2025memorylayers} demonstrated that trainable key-value memory layers can be scaled effectively and show strong factual gains. EMAT \citep{wu2022emat} achieves efficient memory-augmented transformers using Maximum Inner Product Search (MIPS) for fast memory querying. MATTER \citep{lee2024matter} extends this to heterogeneous knowledge sources (paragraphs and QA pairs) with fixed-length neural memories. These techniques use sparse top-k retrieval rather than attention and could be integrated with our chapter-based routing for further scalability.

\textbf{Memory Tokens and Global Memory:} Memory Transformer \citep{burtsev2020memorytransformer} and GMAT \citep{gupta2020gmat} add special memory tokens that participate in self-attention (not cross-attention). ETC \citep{ainslie2020etc} uses global tokens for encoding long and structured inputs. These systems demonstrate that memory-like token mechanisms help with long-context reasoning, but memory tokens are typically dynamic per sequence and use self-attention rather than cross-attention to a separate persistent store. Persistent learned memory vectors integrated directly into attention provide another closely related mechanism for fixed memory access \citep{sukhbaatar2019persistent}.

\textbf{Recurrent Memory Approaches:} The Recurrent Memory Transformer (RMT) \citep{bulatov2022rmt, bulatov2023scaling} augments transformers with memory tokens passed between segments via self-attention. The Associative Recurrent Memory Transformer (ARMT) \citep{rodkin2024armt} extends RMT with linear attention-style associative memory, achieving impressive results on 50M+ token sequences. Block-Recurrent Transformers \citep{hutchins2022blockrecurrent} use block-level recurrence. These approaches focus on segment-level recurrence for long sequences; our memory bank stores learned knowledge accessed via cross-attention without recurrence.

\textbf{Non-Parametric Memory:} Memorizing Transformer \citep{wu2022memorizing} stores actual KV pairs from past forward passes and retrieves via kNN lookup. kNN-LM \citep{khandelwal2019knnlm} retrieves from a datastore of context-target pairs. These non-parametric approaches cache actual activations rather than learning compressed representations. Our memory is learned end-to-end, potentially storing more efficient representations.

\textbf{Entity and Knowledge-Specific Memory:} Entities as Experts \citep{fevry2020entities} stores entity embeddings in dedicated memory slots with sparse access. Facts as Experts \citep{verga2020facts} similarly uses sparse memory over symbolic knowledge with interpretable fact representations. Mention Memory \citep{dearmas2021mention} stores 150M entity mention embeddings accessed via attention. QAMAT \citep{chen2023qamat} uses dedicated memory for question-answering. Knowledge-Infused Self Attention Transformers \citep{roy2023knowledge} systematically infuse external knowledge graphs into transformer components. Relational Memory-Augmented Language Models \citep{liu2022relational} store relation triples rather than token pairs, enabling causal interventions. These demonstrate the value of explicit memory structures but are often task/entity-specific, whereas our approach targets general learned knowledge.

\textbf{Long-Context Transformers:} Transformer-XL \citep{dai2019txl} introduced segment-level recurrence. Compressive Transformers \citep{rae2020compressive} compress past activations. Infini-attention \citep{munkhdalai2024infini} proposed compressive memory within attention. Cached Transformers \citep{zhang2023cached} use gated recurrent cached attention with differentiable memory. HMT \citep{he2024hmt} employs hierarchical memory transformers for efficient long-context processing with memory recall mechanisms. Augmenting Language Models with Long-Term Memory \citep{wang2023longtermemory} adds persistent memory for extended conversations. These focus on extending effective context length, while we focus on persistent learned knowledge.

\textbf{KV Cache Compression and Retention:} H$_2$O \citep{zhang2023h2o} proposes an attention-score-driven eviction strategy that retains high-importance (``heavy hitter'') tokens in the KV cache. StreamingLLM \citep{xiao2023streamingllm} identifies ``attention sinks'' and shows that preserving a small set of sink tokens stabilizes long-context generation. Trellis \citep{karami2025trellis} learns to compress KV memory dynamically at test time with a bounded-memory mechanism. NACL proposes a general KV-cache eviction framework for long-context inference \citep{chen2024nacl}. These works motivate our focus on learning what to store in a persistent, parameterized memory bank rather than relying on heuristic retention or runtime cache compression.

\textbf{Test-Time Memory and Adaptation:} Titans \citep{behrouz2025titans} learns what to memorize at test time with dynamic updates. Larimar \citep{liu2024larimar} provides episodic memory control for LLMs. EM-LLM \citep{fountas2025emllm} proposes episodic memory for large language models via storing and retrieving past episodes. NAMMs \citep{namm2025} use evolutionary optimization for memory decisions. MAC \citep{tack2024mac} enables online adaptation with a memory of amortized contexts using meta-learning. Extended Mind Transformers \citep{klett2024extended} use kNN-based retrieval to attend to external pre-computed memories. These approaches update memory during inference; our memory is fixed after training, providing stable knowledge without runtime overhead.

\textbf{Cross-Attention Architectures:} Perceiver \citep{jaegle2021perceiver} and Perceiver IO \citep{jaegle2021perceiverio} use cross-attention to a latent array as a computational bottleneck. Flamingo \citep{alayrac2022flamingo} uses cross-attention to inject visual information into frozen language models. Our memory cross-attention is conceptually similar but targets persistent knowledge storage rather than multimodal fusion or computational bottlenecks.

\textbf{Learnable Memory Tokens:} Sandler et al. \citep{sandler2022learnable} demonstrated learnable memory tokens for vision transformers, concatenating them to input for self-attention. Shah et al. \citep{shah2025metatokens} introduce learned meta-tokens injected during pre-training as content-based landmarks for improved recall and length generalization. Heterogeneous Memory Augmented Neural Networks \citep{park2023heterogeneous} use learnable memory for OOD generalization. Our work extends learnable token ideas to language models with cross-attention to a separate persistent bank (not self-attention), chapter-based routing, and memory adapter formulations.

\textbf{Explicit Memory Systems:} Neural Turing Machines \citep{graves2014ntm} and Differentiable Neural Computers \citep{graves2016dnc} pioneered differentiable read/write access to an external memory for learning algorithmic behavior. More recently, Memory3 \citep{yang2024memory3} models explicit memory as retrievable model parameters, and Token Turing Machines \citep{hawthorne2024tokenturing} use memory tapes with read/write operations. These represent alternative memory paradigms; our approach uses standard attention mechanics for compatibility with existing transformer infrastructure.

\textbf{Context Compression:} Activation Beacon \citep{activationbeacon2024}, IC-Former \citep{icformer2024}, and In-Context Autoencoder (ICAE) \citep{icae2023} compress long contexts into shorter representations, and learned context auto-compressors, compressed context memory, and semantic-anchor based compression study closely related settings \citep{chevalier2023autocompressors, kim2024ccm, liu2025semanticanchors}. Our context bank proposal incorporates similar compression ideas but maintains a persistent store across inference sessions with importance-weighted consolidation, combining VAE-based compression with clustering-based retrieval and memory consolidation mechanisms.

\textbf{Retrieval-Augmented Methods:} RAG \citep{lewis2020rag} and RETRO \citep{borgeaud2021retro} augment models with retrieved text from external corpora. These store memory as text requiring re-encoding; our approach stores learned latent representations that may be more compact and task-appropriate.

\textbf{Routing and Sparse Attention:} Routing Transformer \citep{roy2021routing} uses clustering for content-based sparse attention. MoE architectures \citep{shazeer2017moe, fedus2022switch, zhou2022moe} use token-wise routing to experts, with variants like expert choice routing providing alternative routing mechanisms. MoM and memory-conditioned routing mechanisms provide related precedents for selecting among multiple memory components \citep{du2025mom, zhang2021neuralrouting}. Our chapter routing applies MoE-style routing to memory chapters, a novel application that enables scaling memory banks while maintaining attention-based access.

\textbf{Parameter-Efficient Fine-Tuning:} Adapters \citep{houlsby2019adapters} provide a modular approach to parameter-efficient transfer by inserting small trainable blocks into frozen models. Prefix-tuning \citep{li2021prefix} and P-Tuning v2 \citep{liu2021ptuningv2} add learned continuous prompts attended via self-attention, which is conceptually close to learnable tokens. LoRA \citep{hu2022lora} provides a strong low-rank adaptation baseline. Discrete key-value adaptor and bottleneck methods such as GRACE \citep{hartvigsen2023grace} and the Discrete Key-Value Bottleneck \citep{trauble2023dkvb} similarly localize updates into a small memory-like interface, which is conceptually aligned with memory-as-adaptor designs. MemLoRA distills expert adapters for on-device memory systems, providing a related perspective on memory-centric adapters under deployment constraints \citep{bini2025memlora}. SPARTAN \citep{deshpande2022spartan} uses sparse hierarchical memory for parameter-efficient adaptation. TRIME \citep{zhong2022trime} trains language models with memory augmentation using in-batch examples as accessible memory. Our memory adapters differ in using cross-attention to a separate memory store with explicit capacity scaling via chapters.

\subsection{Summary of Distinctions}

Several components of our proposal have close precedents, including persistent learned memory vectors in attention \citep{sukhbaatar2019persistent} and internal memory pools that can be updated over time \citep{wang2024memoryllm, wang2025mplus}. Our contribution is to articulate and evaluate a specific combination focused on cross-attention to an explicit learned bank, chapter-based routing for scalable access, and a PEFT instantiation of memory as adapters compared against standard baselines.

\section{Novelty Summary}

The novel contributions of this proposal include:

\begin{enumerate}
    \item \textbf{Learnable cross-attention memory banks} for language transformers, with analysis of shared vs. per-layer configurations and the vector-space alignment property of projection matrices.
    
    \item \textbf{Chapter-based routing} for scaling memory bank size, including analysis of token-level routing challenges and a proposed custom CUDA kernel solution.
    
    \item \textbf{Memory adapters} as a parameter-efficient fine-tuning method, with low-rank and quantization variants.
    
    \item \textbf{Comprehensive experimental framework} comparing memory adapters against established PEFT baselines (e.g., adapters and prefix-tuning) and full fine-tuning.
    
    \item \textbf{Dynamic context bank} concept with VAE compression, clustering-based retrieval, and importance-weighted memory consolidation (future work).
\end{enumerate}

\section{Conclusion}

We have presented a comprehensive proposal for memory-augmented transformers via learnable cross-attention memory banks. The architecture provides an explicit mechanism for knowledge storage and retrieval at the architectural level, addressing a fundamental limitation of standard transformers. Chapter-based routing enables efficient scaling, while memory adapters provide a practical path to experimentation within workshop timelines. The dynamic context bank extends these ideas to enable persistent memory across inference sessions. We believe this direction holds significant promise for improving the knowledge retention and contextual reasoning capabilities of transformer-based language models.

\bibliographystyle{plainnat}

\begin{thebibliography}{99}

\bibitem[Hopfield, 1982]{hopfield1982}
Hopfield, J.J. (1982).
\newblock Neural networks and physical systems with emergent collective computational abilities.
\newblock \emph{Proceedings of the National Academy of Sciences}, 79(8):2554-2558.

\bibitem[Ramsauer et al., 2020]{ramsauer2020hopfield}
Ramsauer, H., Sch{\"a}fl, B., Lehner, J., Seidl, P., Widrich, M., Adler, T., Gruber, L., Holzleitner, M., Pavlovi{\'c}, M., Sandve, G.K., Greiff, V., Kreil, D., Kopp, M., Klambauer, G., Brandstetter, J., and Hochreiter, S. (2020).
\newblock Hopfield networks is all you need.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \url{https://arxiv.org/abs/2008.02217}

\bibitem[Vaswani et al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 30.
\newblock \url{https://arxiv.org/abs/1706.03762}

\bibitem[Lewis et al., 2020]{lewis2020rag}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K{\"u}ttler, H., Lewis, M., Yih, W., Rockt{\"a}schel, T., Riedel, S., and Kiela, D. (2020).
\newblock Retrieval-augmented generation for knowledge-intensive NLP tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 33.
\newblock \url{https://arxiv.org/abs/2005.11401}

\bibitem[Wu et al., 2020]{wu2020memformer}
Wu, Q., Lan, Z., Gu, J., and Yu, Z. (2020).
\newblock Memformer: The memory-augmented transformer.
\newblock \emph{arXiv preprint arXiv:2010.06891}.
\newblock \url{https://arxiv.org/abs/2010.06891}

\bibitem[Bulatov et al., 2022]{bulatov2022rmt}
Bulatov, A., Kuratov, Y., and Burtsev, M.S. (2022).
\newblock Recurrent memory transformer.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 35.
\newblock \url{https://arxiv.org/abs/2207.06881}

\bibitem[Bulatov et al., 2023]{bulatov2023scaling}
Bulatov, A., Kuratov, Y., and Burtsev, M.S. (2023).
\newblock Scaling transformer to 1M tokens and beyond with RMT.
\newblock \emph{arXiv preprint arXiv:2304.11062}.
\newblock \url{https://arxiv.org/abs/2304.11062}

\bibitem[Wu et al., 2022]{wu2022memorizing}
Wu, Y., Rabe, M.N., Hutchins, D., and Szegedy, C. (2022).
\newblock Memorizing transformers.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \url{https://arxiv.org/abs/2203.08913}

\bibitem[Sandler et al., 2022]{sandler2022learnable}
Sandler, M., Zhmoginov, A., Vladymyrov, M., and Jackson, A. (2022).
\newblock Fine-tuning image transformers using learnable memory.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern Recognition}.
\newblock \url{https://arxiv.org/abs/2203.15243}

\bibitem[Lample et al., 2019]{lample2019pkm}
Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., and J{\'e}gou, H. (2019).
\newblock Large memory layers with product keys.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 32.
\newblock \url{https://arxiv.org/abs/1907.05242}

\bibitem[Omidi et al., 2025]{memaugsurvey2025}
Omidi, P., Huang, X., Laborieux, A., Nikpour, B., Shi, T., and Eshaghi, A. (2025).
\newblock Memory-augmented transformers: A systematic review from neuroscience principles to technical solutions.
\newblock \emph{arXiv preprint arXiv:2508.10824}.
\newblock \url{https://arxiv.org/abs/2508.10824}

\bibitem[Shazeer et al., 2017]{shazeer2017moe}
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. (2017).
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \url{https://arxiv.org/abs/1701.06538}

\bibitem[Fedus et al., 2022]{fedus2022switch}
Fedus, W., Zoph, B., and Shazeer, N. (2022).
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23(120):1-39.
\newblock \url{https://arxiv.org/abs/2101.03961}


\bibitem[Demircigil et al., 2017]{demircigil2017huge}
Demircigil, M., Heusel, J., L{\"o}we, M., Upgang, S., and Vermet, F. (2017).
\newblock On a model of associative memory with huge storage capacity.
\newblock \emph{Journal of Statistical Physics}, 168(2):288--299.

\bibitem[Krotov and Hopfield, 2016]{krotov2016dense}
Krotov, D. and Hopfield, J.J. (2016).
\newblock Dense associative memory for pattern recognition.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 29.
\newblock \url{https://arxiv.org/abs/1606.01164}

\bibitem[Graves et al., 2014]{graves2014ntm}
Graves, A., Wayne, G., and Danihelka, I. (2014).
\newblock Neural Turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}.
\newblock \url{https://arxiv.org/abs/1410.5401}

\bibitem[Graves et al., 2016]{graves2016dnc}
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwi{\'n}ska, A., Colmenarejo, S.G., Grefenstette, E., Ramalho, T., Agapiou, J., and others (2016).
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock \emph{Nature}, 538(7626):471--476.

\bibitem[Zhang et al., 2023]{zhang2023h2o}
Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., R{\'e}, C., Barrett, C., Wang, Z., and Chen, B. (2023).
\newblock H$_2$O: Heavy-hitter oracle for efficient generative inference of large language models.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 36.
\newblock \url{https://arxiv.org/abs/2306.14048}

\bibitem[Xiao et al., 2023]{xiao2023streamingllm}
Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. (2023).
\newblock Efficient streaming language models with attention sinks.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \url{https://arxiv.org/abs/2309.17453}

\bibitem[Shah et al., 2025]{shah2025metatokens}
Shah, A.N., Gupta, K., Ramji, K., and Chaudhari, P. (2025).
\newblock Language modeling with learned meta-tokens.
\newblock \emph{arXiv preprint arXiv:2509.16278}.
\newblock \url{https://arxiv.org/abs/2509.16278}

\bibitem[Karami et al., 2025]{karami2025trellis}
Karami, M., Behrouz, A., Kacham, P., and Mirrokni, V. (2025).
\newblock Trellis: Learning to compress key-value memory in attention models.
\newblock \emph{arXiv preprint arXiv:2512.23852}.
\newblock \url{https://arxiv.org/abs/2512.23852}

\bibitem[Houlsby et al., 2019]{houlsby2019adapters}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. (2019).
\newblock Parameter-efficient transfer learning for NLP.
\newblock In \emph{International Conference on Machine Learning}.
\newblock \url{https://arxiv.org/abs/1902.00751}

\bibitem[Qwen Team, 2024]{qwen2}
Qwen Team (2024).
\newblock Qwen2.5 technical report.
\newblock \emph{arXiv preprint arXiv:2412.15115}.
\newblock \url{https://arxiv.org/abs/2412.15115}

\bibitem[Activation Beacon, 2024]{activationbeacon2024}
Zhang, P., Qian, H., Ye, Q., and Dou, Z. (2024).
\newblock Long context compression with activation beacon.
\newblock \emph{arXiv preprint arXiv:2401.03462}.
\newblock \url{https://arxiv.org/abs/2401.03462}

\bibitem[Wang et al., 2024]{icformer2024}
Wang, X., Chen, Z., Xu, T., Xie, Z., He, Y., and Chen, E. (2024).
\newblock In-context former: Lightning-fast compressing context for large language model.
\newblock In \emph{Findings of EMNLP 2024}.
\newblock \url{https://arxiv.org/abs/2406.13618}

\bibitem[Cetin et al., 2025]{namm2025}
Cetin, E., Palmieri, A., Poli, M., Coda, D., Fang, B., and Tang, L. (2025).
\newblock An evolved universal transformer memory.
\newblock Sakana AI Blog.
\newblock \url{https://sakana.ai/namm/}
\newblock (Accessed: January 2026).

\bibitem[Hwang et al., 2024]{hwang2024transformerfam}
Hwang, D., Wang, W., Huo, Z., Sim, K.C., and Mengibar, P. (2024).
\newblock TransformerFAM: Feedback attention is working memory.
\newblock \emph{arXiv preprint arXiv:2404.09173}.
\newblock \url{https://arxiv.org/abs/2404.09173}

\bibitem[RetrievalAttention, 2024]{retrieval2024}
Liu, D., et al. (2024).
\newblock RetrievalAttention: Accelerating long-context LLM inference via vector retrieval.
\newblock \emph{arXiv preprint arXiv:2409.10516}.
\newblock \url{https://arxiv.org/abs/2409.10516}

\bibitem[Dai et al., 2019]{dai2019txl}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and Salakhutdinov, R. (2019).
\newblock Transformer-XL: Attentive language models beyond a fixed-length context.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}.
\newblock \url{https://arxiv.org/abs/1901.02860}

\bibitem[Ge et al., 2023]{icae2023}
Ge, T., Hu, J., Li, L., Qiu, X., Huang, X., and Si, L. (2023).
\newblock In-context autoencoder for context compression in a large language model.
\newblock \emph{arXiv preprint arXiv:2307.06945}.
\newblock \url{https://arxiv.org/abs/2307.06945}

\bibitem[Zhou et al., 2022]{zhou2022moe}
Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V., Dai, A., Chen, Z., Le, Q., and Laudon, J. (2022).
\newblock Mixture-of-experts with expert choice routing.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 35.
\newblock \url{https://arxiv.org/abs/2202.09368}

\bibitem[Rae et al., 2020]{rae2020compressive}
Rae, J.W., Potapenko, A., Jayakumar, S.M., Hillier, C., and Lillicrap, T.P. (2020).
\newblock Compressive transformers for long-range sequence modelling.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \url{https://arxiv.org/abs/1911.05507}

\bibitem[Munkhdalai et al., 2024]{munkhdalai2024infini}
Munkhdalai, T., Faruqui, M., and Gopal, S. (2024).
\newblock Leave no context behind: Efficient infinite context transformers with infini-attention.
\newblock \emph{arXiv preprint arXiv:2404.07143}.
\newblock \url{https://arxiv.org/abs/2404.07143}

\bibitem[Behrouz et al., 2025]{behrouz2025titans}
Behrouz, A., Zhong, P., and Mirrokni, V. (2025).
\newblock Titans: Learning to memorize at test time.
\newblock \emph{arXiv preprint arXiv:2501.00663}.
\newblock \url{https://arxiv.org/abs/2501.00663}

\bibitem[Khandelwal et al., 2019]{khandelwal2019knnlm}
Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M. (2019).
\newblock Generalization through memorization: Nearest neighbor language models.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \url{https://arxiv.org/abs/1911.00172}

\bibitem[Borgeaud et al., 2021]{borgeaud2021retro}
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J.-B., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paez, P., Sheridan, G., Landon, J., and Sifre, L. (2021).
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{International Conference on Machine Learning}.
\newblock \url{https://arxiv.org/abs/2112.04426}

\bibitem[Berges et al., 2025]{berges2025memorylayers}
Berges, V.-P., Oguz, B., Haziza, D., Yih, W.-t., Zettlemoyer, L., and Ghosh, G. (2025).
\newblock Memory layers at scale.
\newblock In \emph{International Conference on Machine Learning}.
\newblock \url{https://arxiv.org/abs/2412.09764}

\bibitem[Burtsev et al., 2020]{burtsev2020memorytransformer}
Burtsev, M.S., Kuratov, Y., Peganov, A., and Sapunov, G.V. (2020).
\newblock Memory transformer.
\newblock \emph{arXiv preprint arXiv:2006.11527}.
\newblock \url{https://arxiv.org/abs/2006.11527}

\bibitem[Gupta and Berant, 2020]{gupta2020gmat}
Gupta, A. and Berant, J. (2020).
\newblock GMAT: Global memory augmentation for transformers.
\newblock \emph{arXiv preprint arXiv:2006.03274}.
\newblock \url{https://arxiv.org/abs/2006.03274}

\bibitem[Ainslie et al., 2020]{ainslie2020etc}
Ainslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., Ravula, A., Sanghai, S., Wang, Q., and Yang, L. (2020).
\newblock ETC: Encoding long and structured inputs in transformers.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}.
\newblock \url{https://arxiv.org/abs/2004.08483}

\bibitem[Roy et al., 2021]{roy2021routing}
Roy, A., Saffar, M., Vaswani, A., and Grangier, D. (2021).
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 9:53-68.
\newblock \url{https://arxiv.org/abs/2003.05997}

\bibitem[Li and Liang, 2021]{li2021prefix}
Li, X.L. and Liang, P. (2021).
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics}.
\newblock \url{https://arxiv.org/abs/2101.00190}

\bibitem[Liu et al., 2022]{liu2021ptuningv2}
Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., and Tang, J. (2022).
\newblock P-Tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics}.
\newblock \url{https://arxiv.org/abs/2110.07602}

\bibitem[Jaegle et al., 2021a]{jaegle2021perceiver}
Jaegle, A., Gimeno, F., Brock, A., Zisserman, A., Vinyals, O., and Carreira, J. (2021).
\newblock Perceiver: General perception with iterative attention.
\newblock In \emph{International Conference on Machine Learning}.
\newblock \url{https://arxiv.org/abs/2103.03206}

\bibitem[Jaegle et al., 2021b]{jaegle2021perceiverio}
Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ber, D., Lakshminarayanan, B., Zisserman, A., Vinyals, O., and Carreira, J. (2021).
\newblock Perceiver IO: A general architecture for structured inputs \& outputs.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \url{https://arxiv.org/abs/2107.14795}

\bibitem[Alayrac et al., 2022]{alayrac2022flamingo}
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Bi{\'n}kowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. (2022).
\newblock Flamingo: A visual language model for few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 35.
\newblock \url{https://arxiv.org/abs/2204.14198}

\bibitem[Kang et al., 2025]{kang2025lm2}
Kang, J., Wu, W., Christianos, F., Chan, A.J., Greenlee, F., Thomas, G., Purtorab, M., and Toulis, A. (2025).
\newblock LM2: Large memory models.
\newblock \emph{arXiv preprint arXiv:2502.06049}.
\newblock \url{https://arxiv.org/abs/2502.06049}

\bibitem[DeepSeek, 2026]{deepseek2026engram}
DeepSeek-AI (2026).
\newblock Engram: Conditional memory via scalable lookup: A new axis of sparsity for large language models.
\newblock \emph{arXiv preprint arXiv:2601.07372}.
\newblock \url{https://arxiv.org/abs/2601.07372}

\bibitem[Rodkin et al., 2024]{rodkin2024armt}
Rodkin, I., Kuratov, Y., Bulatov, A., and Burtsev, M. (2024).
\newblock Associative recurrent memory transformer.
\newblock In \emph{ICML 2024 Workshop on Next Generation of Sequence Modeling Architectures}.
\newblock \url{https://arxiv.org/abs/2407.04841}

\bibitem[Hutchins et al., 2022]{hutchins2022blockrecurrent}
Hutchins, D., Schlag, I., Wu, Y., Dyer, E., and Neyshabur, B. (2022).
\newblock Block-recurrent transformers.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 35.
\newblock \url{https://arxiv.org/abs/2203.07852}

\bibitem[Fevry et al., 2020]{fevry2020entities}
Fevry, T., Soares, L.B., FitzGerald, N., Choi, E., and Kwiatkowski, T. (2020).
\newblock Entities as experts: Sparse memory access with entity supervision.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing}.
\newblock \url{https://arxiv.org/abs/2004.07202}

\bibitem[de Armas et al., 2021]{dearmas2021mention}
de Armas, R.M., Martins, B., and Calado, P. (2021).
\newblock Mention memory: Incorporating textual knowledge into transformers through entity mention attention.
\newblock \emph{arXiv preprint arXiv:2110.06176}.
\newblock \url{https://arxiv.org/abs/2110.06176}

\bibitem[Chen et al., 2023]{chen2023qamat}
Chen, W., Pat, A., and Roth, D. (2023).
\newblock Augmenting pre-trained language models with QA-memory for open-domain question answering.
\newblock In \emph{Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics}.
\newblock \url{https://aclanthology.org/2023.eacl-main.117/}

\bibitem[Wang et al., 2023]{wang2023longtermemory}
Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F. (2023).
\newblock Augmenting language models with long-term memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 36.
\newblock \url{https://arxiv.org/abs/2306.07174}

\bibitem[Liu et al., 2024]{liu2024larimar}
Liu, P., Zhang, Y., and Weld, D.S. (2024).
\newblock Larimar: Large language models with episodic memory control.
\newblock \emph{arXiv preprint arXiv:2403.11901}.
\newblock \url{https://arxiv.org/abs/2403.11901}

\bibitem[Park et al., 2023]{park2023heterogeneous}
Park, S., Kim, J., and Lee, J. (2023).
\newblock Heterogeneous memory augmented neural networks.
\newblock \emph{arXiv preprint arXiv:2310.10909}.
\newblock \url{https://arxiv.org/abs/2310.10909}

\bibitem[Yang et al., 2024]{yang2024memory3}
Yang, H., et al. (2024).
\newblock Memory3: Language modeling with explicit memory.
\newblock \emph{arXiv preprint}.
\newblock \url{https://arxiv.org/abs/2407.01178}

\bibitem[Hawthorne et al., 2024]{hawthorne2024tokenturing}
Hawthorne, C., et al. (2024).
\newblock Token turing machines are efficient vision models.
\newblock \emph{arXiv preprint arXiv:2409.07613}.
\newblock \url{https://arxiv.org/abs/2409.07613}

\bibitem[He et al., 2024]{he2024hmt}
He, Z., Cao, Y., Qin, Z., Prakriya, N., Sun, Y., and Cong, J. (2024).
\newblock HMT: Hierarchical memory transformer for efficient long context language processing.
\newblock In \emph{Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics}.
\newblock \url{https://arxiv.org/abs/2405.06067}

\bibitem[Klett and Ahle, 2024]{klett2024extended}
Klett, P. and Ahle, T. (2024).
\newblock Extended mind transformers.
\newblock \emph{arXiv preprint arXiv:2406.02332}.
\newblock \url{https://arxiv.org/abs/2406.02332}

\bibitem[Lee et al., 2024]{lee2024matter}
Lee, D., Prakash, C.S., FitzGerald, J., and Lehmann, J. (2024).
\newblock MATTER: Memory-augmented transformer using heterogeneous knowledge sources.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2024}, pages 16110--16121.
\newblock \url{https://arxiv.org/abs/2406.04670}

\bibitem[Liu et al., 2022]{liu2022relational}
Liu, Q., Yogatama, D., and Blunsom, P. (2022).
\newblock Relational memory-augmented language models.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:555--572.
\newblock \url{https://arxiv.org/abs/2201.09680}

\bibitem[Roy, 2023]{roy2023knowledge}
Roy, K. (2023).
\newblock Knowledge-infused self attention transformers.
\newblock \emph{arXiv preprint arXiv:2306.13501}.
\newblock \url{https://arxiv.org/abs/2306.13501}

\bibitem[Deshpande et al., 2022]{deshpande2022spartan}
Deshpande, A., Sultan, M.A., Ferritto, A., Kalyan, A., Narasimhan, K., and Sil, A. (2022).
\newblock SPARTAN: Sparse hierarchical memory for parameter-efficient transformers.
\newblock \emph{arXiv preprint arXiv:2211.16634}.
\newblock \url{https://arxiv.org/abs/2211.16634}

\bibitem[Tack et al., 2024]{tack2024mac}
Tack, J., et al. (2024).
\newblock Online adaptation of language models with a memory of amortized contexts.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 37.
\newblock \url{https://arxiv.org/abs/2403.04317}

\bibitem[Verga et al., 2020]{verga2020facts}
Verga, P., Sun, H., Soares, L.B., and Cohen, W. (2020).
\newblock Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}.
\newblock \url{https://arxiv.org/abs/2007.00849}

\bibitem[Wu et al., 2022]{wu2022stateful}
Wu, Q. and Yu, Z. (2022).
\newblock Stateful memory-augmented transformers for efficient dialogue modeling.
\newblock In \emph{Findings of the Association for Computational Linguistics: EACL 2024}, pages 853--867.
\newblock \url{https://arxiv.org/abs/2209.07634}

\bibitem[Wu et al., 2022]{wu2022emat}
Wu, Y., Zhao, Y., Hu, B., Minervini, P., Stenetorp, P., and Riedel, S. (2022).
\newblock An efficient memory-augmented transformer for knowledge-intensive NLP tasks.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 5184--5196.
\newblock \url{https://arxiv.org/abs/2210.16773}

\bibitem[Zhang et al., 2023]{zhang2023cached}
Zhang, Z., Shao, W., Ge, Y., Wang, X., Gu, J., and Luo, P. (2023).
\newblock Cached transformers: Improving transformers with differentiable memory cache.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 38(15):16935--16943.
\newblock \url{https://arxiv.org/abs/2312.12742}

\bibitem[Zhong et al., 2022]{zhong2022trime}
Zhong, Z., Lei, T., and Chen, D. (2022).
\newblock Training language models with memory augmentation.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 5657--5673.
\newblock \url{https://arxiv.org/abs/2205.12674}

\bibitem[Sukhbaatar et al., 2019]{sukhbaatar2019persistent}
Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A. (2019).
\newblock Augmenting self-attention with persistent memory.
\newblock \emph{arXiv preprint arXiv:1907.01470}.
\newblock \url{https://arxiv.org/abs/1907.01470}

\bibitem[Geva et al., 2021]{geva2021ffnkv}
Geva, M., Schuster, R., Berant, J., and Levy, O. (2021).
\newblock Transformer feed-forward layers are key-value memories.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}.
\newblock \url{https://arxiv.org/abs/2012.14913}

\bibitem[Hu et al., 2022]{hu2022lora}
Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., and Chen, W. (2022).
\newblock LoRA: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \url{https://arxiv.org/abs/2106.09685}

\bibitem[Hartvigsen et al., 2023]{hartvigsen2023grace}
Hartvigsen, T., Yin, F., Hartman, M., Mahoney, M., and Van~Durme, B. (2023).
\newblock Aging with GRACE: Lifelong model editing with discrete key-value adaptors.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 36.
\newblock \url{https://arxiv.org/abs/2305.15031}

\bibitem[Tr{\"a}uble et al., 2023]{trauble2023dkvb}
Tr{\"a}uble, F., Fuchs, F.B., Gelly, S., and Luisier, F. (2023).
\newblock Discrete key-value bottleneck.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}.
\newblock \url{https://arxiv.org/abs/2305.18353}

\bibitem[Chevalier et al., 2023]{chevalier2023autocompressors}
Chevalier, A., Sulem, E., and Choshen, L. (2023).
\newblock Adapting language models to compress contexts.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}.
\newblock \url{https://aclanthology.org/2023.emnlp-main.632/}

\bibitem[Kim et al., 2024]{kim2024ccm}
Kim, J.H., Ma, J., Lausen, L., Tan, C., Zhang, J., and Cui, Y. (2024).
\newblock Compressed context memory for online language model interaction.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \url{https://arxiv.org/abs/2312.03414}

\bibitem[Wang et al., 2024]{wang2024memoryllm}
Wang, Y., et al. (2024).
\newblock MEMORYLLM: Towards self-updatable large language models.
\newblock In \emph{Proceedings of the 41st International Conference on Machine Learning}.
\newblock \url{https://arxiv.org/abs/2402.04624}

\bibitem[Wang et al., 2025]{wang2025mplus}
Wang, Y., et al. (2025).
\newblock M+: Extending MemoryLLM with scalable long-term memory.
\newblock \emph{arXiv preprint arXiv:2502.00592}.
\newblock \url{https://arxiv.org/abs/2502.00592}

\bibitem[Du et al., 2025]{du2025mom}
Du, J., Sun, W., Lan, D., Hu, J., and Cheng, Y. (2025).
\newblock MoM: Linear sequence modeling with mixture-of-memories.
\newblock \emph{arXiv preprint arXiv:2502.13685}.
\newblock \url{https://arxiv.org/abs/2502.13685}

\bibitem[Zhang et al., 2021]{zhang2021neuralrouting}
Zhang, K., Li, Z., Li, Z., Liu, W., and Sato, Y. (2021).
\newblock Neural routing by memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume 34.

\bibitem[Bini et al., 2025]{bini2025memlora}
Bini, L., Bohdal, S., Michieli, U., Akata, Z., and Ceritli, E.C. (2025).
\newblock MemLoRA: Distilling expert adapters for on-device memory systems.
\newblock \emph{arXiv preprint arXiv:2512.04763}.
\newblock \url{https://arxiv.org/abs/2512.04763}

\bibitem[Fountas et al., 2025]{fountas2025emllm}
Fountas, N., De~Cao, N., Wolff, T., and Peyrard, M. (2025).
\newblock EM-LLM: Episodic memory for large language models.
\newblock In \emph{International Conference on Learning Representations}.
\newblock \url{https://arxiv.org/abs/2407.09450}

\bibitem[Liu et al., 2025]{liu2025semanticanchors}
Liu, X., Zhao, R., Huang, P., Liu, X., Xiao, J., Xiao, C., Xiao, T., Gao, S., Yu, Z., and Zhu, J. (2025).
\newblock Autoencoding-free context compression for LLMs via contextual semantic anchors.
\newblock \emph{arXiv preprint arXiv:2510.08907}.
\newblock \url{https://arxiv.org/abs/2510.08907}

\bibitem[Chen et al., 2024]{chen2024nacl}
Chen, Y., Wang, G., Shang, J., Cui, S., Zhang, Z., Liu, T., Wang, S., Sun, Y., Yu, D., and Wu, H. (2024).
\newblock NACL: A general and effective KV cache eviction framework for LLMs at inference time.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}.
\newblock \url{https://arxiv.org/abs/2408.03675}

\end{thebibliography}

\end{document}
